{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Stable Baselines3 - Monitor Training and Plotting\n",
        "\n",
        "Github Repo: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.\n",
        "\n",
        "It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OSovFsPUgkEL"
      },
      "outputs": [],
      "source": [
        "# for autoformatting\n",
        "# %load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "outputs": [],
      "source": [
        "#apt-get update && apt-get install swig cmake\n",
        "#!pip install box2d-py\n",
        "#!pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Import policy, RL agent, Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.callbacks import BaseCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Define a Callback Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "outputs": [],
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "            # Retrieve training reward\n",
        "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
        "            if len(x) > 0:\n",
        "                # Mean training reward over the last 100 episodes\n",
        "                mean_reward = np.mean(y[-100:])\n",
        "                if self.verbose > 0:\n",
        "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                    print(\n",
        "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
        "                    )\n",
        "\n",
        "                # New best model, you could save the agent here\n",
        "                if mean_reward > self.best_mean_reward:\n",
        "                    self.best_mean_reward = mean_reward\n",
        "                    # Example for saving best model\n",
        "                    if self.verbose > 0:\n",
        "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                    self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c8VHsiXC7dL"
      },
      "source": [
        "## Create and wrap the environment\n",
        "\n",
        "We will be using Lunar Lander environment with continuous actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmxIq5UeC3Nj",
        "outputId": "f43b4807-c340-4d61-bd41-f893280a361b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:521: UserWarning: \u001b[33mWARN: Using the latest versioned environment `Acrobot-v1` instead of the unversioned environment `Acrobot`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "# Create log dir\n",
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make(\"Acrobot-v1\")\n",
        "# Logs will be saved in log_dir/monitor.csv\n",
        "env = Monitor(env, log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80OxZ_uMDd4J"
      },
      "source": [
        "## Define and train the PPO agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaOPfOrwWEP4",
        "outputId": "f3f9c8a3-95f2-41d1-b7f7-9f014a54e77c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -inf - Last mean reward per episode: -448.00\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -448.00 - Last mean reward per episode: -474.00\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 474      |\n",
            "|    ep_rew_mean     | -474     |\n",
            "| time/              |          |\n",
            "|    fps             | 943      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Num timesteps: 3000\n",
            "Best mean reward: -448.00 - Last mean reward per episode: -482.67\n",
            "Num timesteps: 4000\n",
            "Best mean reward: -448.00 - Last mean reward per episode: -487.00\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 487          |\n",
            "|    ep_rew_mean          | -487         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 732          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019200409 |\n",
            "|    clip_fraction        | 0.00156      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | -0.0409      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 20.8         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00262     |\n",
            "|    value_loss           | 143          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5000\n",
            "Best mean reward: -448.00 - Last mean reward per episode: -464.10\n",
            "Num timesteps: 6000\n",
            "Best mean reward: -448.00 - Last mean reward per episode: -470.08\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 473         |\n",
            "|    ep_rew_mean          | -472        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 685         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009275975 |\n",
            "|    clip_fraction        | 0.0455      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.113       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 19.8        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00497    |\n",
            "|    value_loss           | 108         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7000\n",
            "Best mean reward: -448.00 - Last mean reward per episode: -448.93\n",
            "Num timesteps: 8000\n",
            "Best mean reward: -448.00 - Last mean reward per episode: -448.18\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 446          |\n",
            "|    ep_rew_mean          | -446         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 603          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071992204 |\n",
            "|    clip_fraction        | 0.0569       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | -0.00308     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17           |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00664     |\n",
            "|    value_loss           | 103          |\n",
            "------------------------------------------\n",
            "Num timesteps: 9000\n",
            "Best mean reward: -448.00 - Last mean reward per episode: -437.80\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 10000\n",
            "Best mean reward: -437.80 - Last mean reward per episode: -414.75\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 407          |\n",
            "|    ep_rew_mean          | -407         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 602          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 16           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068396386 |\n",
            "|    clip_fraction        | 0.0161       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 0.184        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 21           |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00298     |\n",
            "|    value_loss           | 82.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 11000\n",
            "Best mean reward: -414.75 - Last mean reward per episode: -390.32\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 12000\n",
            "Best mean reward: -390.32 - Last mean reward per episode: -370.72\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 361         |\n",
            "|    ep_rew_mean          | -360        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 20          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007947139 |\n",
            "|    clip_fraction        | 0.064       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.163       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 25.9        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.00516    |\n",
            "|    value_loss           | 80.2        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 13000\n",
            "Best mean reward: -370.72 - Last mean reward per episode: -337.18\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 14000\n",
            "Best mean reward: -337.18 - Last mean reward per episode: -321.26\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 317          |\n",
            "|    ep_rew_mean          | -316         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 560          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056750895 |\n",
            "|    clip_fraction        | 0.051        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.997       |\n",
            "|    explained_variance   | 0.232        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 26.3         |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.00547     |\n",
            "|    value_loss           | 79.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 15000\n",
            "Best mean reward: -321.26 - Last mean reward per episode: -305.24\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 16000\n",
            "Best mean reward: -305.24 - Last mean reward per episode: -294.31\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 291         |\n",
            "|    ep_rew_mean          | -290        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 533         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 30          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008156989 |\n",
            "|    clip_fraction        | 0.0615      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.953      |\n",
            "|    explained_variance   | 0.321       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.1        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.00512    |\n",
            "|    value_loss           | 76.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 17000\n",
            "Best mean reward: -294.31 - Last mean reward per episode: -282.05\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 18000\n",
            "Best mean reward: -282.05 - Last mean reward per episode: -273.38\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 271          |\n",
            "|    ep_rew_mean          | -270         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 539          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 34           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065324134 |\n",
            "|    clip_fraction        | 0.045        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.92        |\n",
            "|    explained_variance   | 0.672        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.81         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00524     |\n",
            "|    value_loss           | 49.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 19000\n",
            "Best mean reward: -273.38 - Last mean reward per episode: -263.94\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 20000\n",
            "Best mean reward: -263.94 - Last mean reward per episode: -254.79\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 251          |\n",
            "|    ep_rew_mean          | -250         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 544          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 37           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070775924 |\n",
            "|    clip_fraction        | 0.0482       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.878       |\n",
            "|    explained_variance   | 0.773        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 13.3         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00655     |\n",
            "|    value_loss           | 45.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 21000\n",
            "Best mean reward: -254.79 - Last mean reward per episode: -247.04\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 22000\n",
            "Best mean reward: -247.04 - Last mean reward per episode: -239.78\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 237         |\n",
            "|    ep_rew_mean          | -236        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 533         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 42          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007829961 |\n",
            "|    clip_fraction        | 0.0723      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.81       |\n",
            "|    explained_variance   | 0.785       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 19.2        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00654    |\n",
            "|    value_loss           | 45.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 23000\n",
            "Best mean reward: -239.78 - Last mean reward per episode: -231.34\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 24000\n",
            "Best mean reward: -231.34 - Last mean reward per episode: -199.90\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 184         |\n",
            "|    ep_rew_mean          | -183        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 538         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 45          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008990269 |\n",
            "|    clip_fraction        | 0.0765      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.734      |\n",
            "|    explained_variance   | 0.879       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 18.5        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0069     |\n",
            "|    value_loss           | 40.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 25000\n",
            "Best mean reward: -199.90 - Last mean reward per episode: -172.43\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 26000\n",
            "Best mean reward: -172.43 - Last mean reward per episode: -161.64\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 156         |\n",
            "|    ep_rew_mean          | -155        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 542         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005284597 |\n",
            "|    clip_fraction        | 0.0649      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.708      |\n",
            "|    explained_variance   | 0.859       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 14.8        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.00604    |\n",
            "|    value_loss           | 44.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 27000\n",
            "Best mean reward: -161.64 - Last mean reward per episode: -151.59\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 28000\n",
            "Best mean reward: -151.59 - Last mean reward per episode: -144.85\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 140         |\n",
            "|    ep_rew_mean          | -139        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 544         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 52          |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010772241 |\n",
            "|    clip_fraction        | 0.0521      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.637      |\n",
            "|    explained_variance   | 0.805       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.89        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.00569    |\n",
            "|    value_loss           | 32.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 29000\n",
            "Best mean reward: -144.85 - Last mean reward per episode: -138.71\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 30000\n",
            "Best mean reward: -138.71 - Last mean reward per episode: -132.52\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | -127         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 527          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 58           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043443292 |\n",
            "|    clip_fraction        | 0.0365       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.642       |\n",
            "|    explained_variance   | 0.904        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 12.1         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00378     |\n",
            "|    value_loss           | 29.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 31000\n",
            "Best mean reward: -132.52 - Last mean reward per episode: -124.46\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 32000\n",
            "Best mean reward: -124.46 - Last mean reward per episode: -120.72\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | -119         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 531          |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 61           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041946257 |\n",
            "|    clip_fraction        | 0.0375       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.55        |\n",
            "|    explained_variance   | 0.862        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 18.5         |\n",
            "|    n_updates            | 150          |\n",
            "|    policy_gradient_loss | -0.00432     |\n",
            "|    value_loss           | 34.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 33000\n",
            "Best mean reward: -120.72 - Last mean reward per episode: -117.34\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 34000\n",
            "Best mean reward: -117.34 - Last mean reward per episode: -112.85\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 113         |\n",
            "|    ep_rew_mean          | -112        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 534         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 65          |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003939894 |\n",
            "|    clip_fraction        | 0.0436      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.556      |\n",
            "|    explained_variance   | 0.892       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11          |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.00438    |\n",
            "|    value_loss           | 28.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 35000\n",
            "Best mean reward: -112.85 - Last mean reward per episode: -111.47\n",
            "Saving new best model to /tmp/gym/best_model.zip\n",
            "Num timesteps: 36000\n",
            "Best mean reward: -111.47 - Last mean reward per episode: -111.48\n"
          ]
        }
      ],
      "source": [
        "# Create action noise because TD3 and DDPG use a deterministic policy\n",
        "#n_actions = env.action_space.shape[-1]\n",
        "#action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "# Create the callback: check every 1000 steps\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
        "# Create RL model\n",
        "model = PPO('MlpPolicy', env, verbose=1, seed=0)\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=int(5e4), callback=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ4bxRQZDuk1"
      },
      "source": [
        "## Plotting helpers\n",
        "\n",
        "Stable Baselines3 has some built-in plotting helper, that you can find in `stable_baselines3.common.results_plotter`. However, to show how to do it yourself, we are going to use custom plotting functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_kMEHmJm3P3"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common import results_plotter\n",
        "\n",
        "# Helper from the library\n",
        "results_plotter.plot_results(\n",
        "    [log_dir], 1e5, results_plotter.X_TIMESTEPS, \"PPO Acrobot-v1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPXYbV39DiCj"
      },
      "outputs": [],
      "source": [
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, \"valid\")\n",
        "\n",
        "\n",
        "def plot_results(log_folder, title=\"Learning Curve\"):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "    x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
        "    y = moving_average(y, window=50)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y) :]\n",
        "\n",
        "    fig = plt.figure(title)\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel(\"Number of Timesteps\")\n",
        "    plt.ylabel(\"Rewards\")\n",
        "    plt.title(title + \" Smoothed\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQXx7HiSDt7_"
      },
      "outputs": [],
      "source": [
        "plot_results(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQmsSZUHKNRG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "monitor_training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}